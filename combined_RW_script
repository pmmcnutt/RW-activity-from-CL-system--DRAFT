# -*- coding: utf-8 -*-"""Created on Wed Dec 21 20:25:56 2016@author: Patrick McNutt"""##################################################################### importing files and setting directory####################################################################from __future__ import print_functionimport csvimport osimport globimport pandas as pdimport numpy as npimport datetimeimport dateutil.parserimport easygui as eg##################################################################### system conditions and versions####################################################################"""  python = 2.7.12  pandas = 0.19.1  numpy = 1.11.2  futures = 3.05  Note that RW files with raw data should be in the same directory as this python script  NO OTHER CSV FILES SHOULD BE IN THAT DIRECTORY!  A subordinate results directory will be created by this script """################################################################################ Define functions that will be used to analyze data###############################################################################def analyze_raw_data():# check if the work directory exists and enumerate the *.csv files in it# initialize an array to store the paths and names of *.csv filenames    listofcsvfiles = []    for file in glob.glob(rundir + '\*.csv'):         listofcsvfiles.append(file)    print ("\nThere are " + str(len(listofcsvfiles)) + " RW data files in the working directory for analysis")    print( "\nAnalyzing raw data and preparing output files for...\n")    datasummary = pd.DataFrame()    # Sequentially analyze each CSV with for loop    RWfile = listofcsvfiles[0]    for RWfile in listofcsvfiles:            with open(RWfile) as csvfile:                reader = csv.reader(csvfile,delimiter=",",quotechar="\"")    # find the header row using enumerate to search for the string 'Time' and store header row                for num, row in enumerate(reader):                    if 'Time' in row:                        break                datarow = num     # print initial dataframe, starting at header row and only keeping certain columns                RWdf1 = pd.read_csv(RWfile, skiprows=datarow, usecols=[0,1,2,3])     # rename columns                RWdf1.columns = ['Interval','Cage', 'Time', 'Counts']     # establish experiment date for output data table                date = RWdf1.iloc[0,2]                expt_date, hour = date.split(" ")     # determine optimal format for strptime (to name output files) will use dateutil     # will convert Time column from string to datetime (note %Y is for 2016, while %y = 16)     # a lambda function allows us to iterate this down the Time column within the dataframe                RWdf1['Time'] = RWdf1['Time'].apply(lambda x:                    dateutil.parser.parse(x))    # generate date in M/D/YYYY format                RWdf1['Day'] = RWdf1['Time'].apply(lambda x:                    datetime.datetime.strftime(x, '%m/%d/%Y'))     # round times up to nearest 5 min and index dataframe using timestamp                RWdf1['Time']=pd.DatetimeIndex(RWdf1['Time']).round('5min')     # remove M/D/Y and Second values from timestamp                RWdf1['Time'] = RWdf1['Time'].apply(lambda x:                    datetime.datetime.strftime(x, '%H:%M'))     # filter based on time to only keep defined periods (e.g.,data from 1930-0425)                RWdf1_masked = RWdf1[(RWdf1.Time > '19:29:59') & (RWdf1.Time < '24:00')]                RWdf2_masked = RWdf1[(RWdf1.Time >= '00:00') & (RWdf1.Time <= '04:25:00')]      # stitch masked data frames together for running wheel and sustained RW data                RWdf = pd.concat([RWdf1_masked, RWdf2_masked], axis = 0)      # set first interval to 1 and replace original interval data with 1-108 interval #s using lambda                Intfirst = RWdf.iloc[0,0]                Intoffset = Intfirst - 1                RWdf['Interval'] = RWdf['Interval'].apply(lambda x:                    x - Intoffset)     # reset index to delete empty rows. Inplace=true means original index overwritten                RWdf.reset_index(drop=True,inplace=True)    ###################################################################    # analyze data in each file using groupby    ###################################################################       # determine basic statistics for each cage using pandas groupby function       # multiply all count data by 0.3048 to convert to distance in meters                grouped = RWdf.groupby(['Cage'])['Counts']#                cage_mean = 0.3048 * grouped.mean()#                cage_stdev = 0.3048 * grouped.std()#                cage_sem = 0.3048 * grouped.sem()                cage_sum = 0.3048 * grouped.sum()#                cage_max = 0.3048 * grouped.max()#                cage_min = 0.3048 * grouped.min()                number_intervals = grouped.count()    # determine number of intervals with >15 counts per cage                grouped2b = grouped.apply(lambda x: (x > 0).sum())                fraction_active = grouped2b/number_intervals#                fraction_inactive = 1 - grouped2b/number_intervals    # get the experimental day                groupedday = RWdf.groupby(['Cage'])['Day']                day = groupedday.first()                day = pd.to_datetime(day, format='%m/%d/%Y')                day = day.dt.strftime('%Y%m%d')       # determine average counts using intervals with > 0 counts, measured per cage       # had to break dataframe into series, applying replace to Counts data to convert anything less than 16 to null values       # After rebuilding the dataframe, redid groupby and analyzed active interval data                cages = RWdf['Cage']                noNullRWdf= RWdf['Counts'].replace(0, np.nan)                grouped3 = pd.concat([cages,noNullRWdf], axis = 1, keys=['Cage','Counts'])                grouped4 = grouped3['Counts'].groupby(grouped3['Cage'])                active_only_mean =  0.06096 * grouped4.mean()         # this corresponds to 0.3048 (ft/m) divided by 5 min, to get velocity in units of m/min#                active_only_stdev = 0.3048 * grouped4.std()#                active_only_sem = 0.3048 * grouped4.sem()#                active_max = 0.3048 * grouped4.max()#                active_min = 0.3048 * grouped4.min()#                active_median = 0.3048 * grouped4.median()       # generate cumulative data for each cage across experimental window                cumdx = grouped.expanding().sum()                cumdx = cumdx.apply(lambda x:                    x * .3048)        # reformat groupby object to a normal dataframe by reindexing                cumdx = cumdx.reset_index()                cumdx.set_index(['level_1'], inplace=True)                cumdx.sort_index(inplace =True)                cumdx.index.name = None                cumdx.columns = ['Cage', 'Cum dx (m)']                del cumdx['Cage']    # insert a new column in dataframe with counts converted to meters using lambda                RWdf['Distance (m)'] = RWdf['Counts'].apply(lambda x:                    x * .3048)    # insert a new column in dataframe with experiment date                filenamefortrimmeddata = str(dateutil.parser.parse(expt_date).strftime('%Y%m%d'))                RWdf['Expt Date'] = filenamefortrimmeddata    # merge with cumdx and replace null values in active_only mean with 0s                RWdata_out = RWdf.merge(cumdx, left_index=True, right_index=True, copy=False)    # prepare and export trimmed data                RWdata_out = RWdata_out[['Cage', 'Expt Date','Interval','Time','Counts','Distance (m)','Cum dx (m)']] # reorder columns in dataframe                RWdata_out = RWdata_out.sort_values(['Cage','Interval']) # sort values by cage and interval                writer = (resultsdir+"\\"+filenamefortrimmeddata+'_RW_output.csv') # export to excel file                RWdata_out.to_csv(path_or_buf=writer,index=False)    # aggregate data for each file to pandas dataframe#                aggresults = pd.concat([day, cage_sum, cage_mean, cage_stdev, cage_sem, cage_max, number_intervals, grouped2b, fraction_active, fraction_inactive, active_only_mean, active_only_stdev, active_only_sem, active_median, active_min],axis=1)#                aggresults.columns = ['day','total dx (m)', 'mean dx per interval (m)', 'SD dx per interval (m)', 'SEM dx per interval (m)', 'max dx during an interval (m)', 'total # intervals', 'number of run intervals','fraction of run intervals', 'fraction of rest intervals', 'mean dx during run intervals (m)', 'SD during run intervals (m)','SEM during run intervals (m)', 'median of run intervals', 'min dx in one run interval (m)']                aggresults = pd.concat([day, cage_sum,  fraction_active, active_only_mean],axis=1)                aggresults.columns = ['day','total dx (m)', 'fraction of run intervals', 'mean dx during run intervals (m)']                datasummary = datasummary.append(aggresults)    # exit for loop and write final file            datasummary = datasummary.reindex() # reindex to pull cages back out of index            datasummary = datasummary.fillna(0)            print ("RW data from",expt_date)            datasummary.to_csv(resultsdir+"\\aggregate_RW_statistics.csv", header=True)            datasummarypivot = pd.read_csv(resultsdir+"\\aggregate_RW_statistics.csv")            datasummarypivot = pd.pivot_table(datasummarypivot, index=["Cage"], columns=['day'])            datasummarypivot.to_csv(resultsdir+"\\aggregate_pivot_RW_statistics.csv")    # there is a pause while the next two functions are executed...to inform user that the script is still running    print ("\nPlease wait while output files are generated")######################################################################################################################################################################################################################################################################################################def concatenate_trimmed_data():    ####################################################################    # starting the second script, "RW analysis append all trimmed files.py"    # this script concatenates the binned trimmed output files produced by the main RW script    # these files should be in the original RW_results directory created by the main script    # the concatenated file will then be reshaped to make graphing easier    ####################################################################    # use os.path.join to make concatenation OS independent    input_files = glob.glob(os.path.join(resultsdir, "*output.csv"))    output = (pd.read_csv(f) for f in input_files)    concat_out = pd.concat(output, ignore_index=True)    # export the trimmed, concatenated file    writer = (resultsdir+"\\concat_data.csv") # export to file    concat_out.to_csv(path_or_buf=writer, mode='w', index=False)    # convert concatenated data to pivot tables for ease of graphing and write to file    reshaped_intdx = pd.pivot_table(concat_out, index=['Cage', 'Expt Date'], columns=['Interval'], values=['Distance (m)'])    reshaped_cumdx = pd.pivot_table(concat_out, index=['Cage', 'Expt Date'], columns=['Interval'], values=['Cum dx (m)'])    writer = (resultsdir+"\\concat_reshaped_interval_data.csv")    writer2 = (resultsdir+"\\concat_reshaped_cum_data.csv")    reshaped_intdx.to_csv(path_or_buf=writer, mode='w')    reshaped_cumdx.to_csv(path_or_buf=writer2, mode='w')    print ("\nConcatenation and reshaping complete.\n")######################################################################################################################################################################################################################################################################################################def calcSessions():    ####################################################################    # Calculate run/rest sessions.    ####################################################################    # open pivot file and reformat data for analysis    RWfile = resultsdir+'\\concat_data.csv'    RWdata = pd.read_csv(RWfile, dtype={'day': object})    RWdata.columns = ['cage', 'day', 'interval', 'time', 'counts', 'distance', 'cumulative_distance']    df = pd.DataFrame(columns = ['cage','day'])         # Dataframe to house all animal's session dataframes    # set initial parameters    currTime = None    prevDist = None    prevTime = None    currDist= 0    # start counter to describe script progression    n = 1    total_n = len(RWdata)/108    resultsDict={}    test = pd.DataFrame()    # iterate over concat data and set values during each iteration    for idx, col in RWdata.iterrows():        currCage = RWdata.iloc[idx,0]        currDay = RWdata.iloc[idx,1]        currinterval = RWdata.iloc[idx,2]        currTime = RWdata.iloc[idx,3]        currDist = RWdata.iloc[idx,5]    # start analysis with first interval for ecah cage and day        if currinterval == 1:                           # establish variables that reset each time the interval = 1            runInt = 0                                  # current sum of minutes in run phase            runDist = 0                                 # current sum of distance in run phase            runMinsList = []                            # list of minutes per run phase            runDistList = []                            # list of distances per run phase            runStartList = []                           # list of run start times            runEndList = []                             # list of run end times            restInt = 0                                 # current sum of minutes in rest phase            restMinsList = []                           # current list of minutes for each rest phase            restStartList = []                          # list of rest start times            restEndList = []                            # list of rest end times            if currDist > 0:                            # run phase in first interval                runStartList.append(currTime)                runInt += 1                runDist += currDist            else:                                       # rest phase in first interval                restStartList.append(currTime)                restInt += 1                runStartList.append(None)               # If the night starts with a rest phase, then append empty values for the run phase                runEndList.append(None)                runMinsList.append(None)                runDistList.append(None)            prevDist = currDist            prevTime = currTime        elif (currinterval >1) & (currinterval < 108):            if prevDist == 0:                           # If interval > 1 and animal was resting last interval                if currDist == 0:                       # and resting this interval, continue rest session                    restInt += 1                    prevDist = currDist                else:                                   # or running this interval, end rest session                    restEndList.append(prevTime)                    restMinsList.append(restInt)                    restMins = 0                        # reset accumulated rest minute total                    runStartList.append(currTime)       # start new run phase                    runInt = 1                          # start running phase counter                    runDist = currDist            elif prevDist > 0:                          # If animal was running last interval                if currDist > 0:                        # and running this minute, continue run session                    runInt += 1                    runDist += currDist                    prevDist = currDist                else:                                   # or resting this interval, end run session                    runMinsList.append(runInt)                    runDistList.append(runDist)                    runEndList.append(prevTime)                    runInt = 0                          # reset accumulated run phase minutes                    restStartList.append(currTime)                    restInt = 1                         # start resting phase counter            prevDist = currDist            prevTime = currTime        else:                                           # If it is interval = 108            if prevDist == 0:                           # and mouse did not run in previous interval                if currDist == 0:                       # and if mouse did not run in the final interval, then                    restInt += 1                        # increment resting interval count by one                    restEndList.append(None)            # put empty value for resting end time                    restMinsList.append(restInt)        # append resting interval count to restMinList                else:                                   # otherwise, if mouse did run in the final interval                    runInt = 1                          # make running interval duration = 1                    runDist = currDist                  # make runnign distance = to distance run in the final interval                    runStartList.append(currTime)                    runMinsList.append(runInt)                    runDistList.append(runDist)                    runEndList.append(None)             # since mouse finishes running, there is no end time for current running session                    restEndList.append(prevTime)        # the previous resting interval ended on inteval 107            else:                                       # If animal ran in previous interval                if currDist == 0:                       # and animal did not run in the final interval                    restInt = 1                         # resting interval = 1                    runEndList.append(None)             # put null value for the end of the resting session                    restMinsList.append(restInt)                else:                                   # otherwise, if mouse ran in final interval                    runInt += 1                         # increment running session by 1                    runDist += currDist                 # sum total distance run during running session                    runMinsList.append(runInt)                    runDistList.append(runDist)        # Create a dictionary of all the lists to form a new data frame        # not used, but useful to remember:  test = pd.DataFrame.from_dict(resultsDict, orient='columns')            resultsDict = {'run_start':runStartList, 'run_end':runEndList,                           'run_intervals':runMinsList,'run_dist(m)':runDistList,                           'rest_start':restStartList, 'rest_end':restEndList,                           'rest_intervals':restMinsList                           }            test = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in resultsDict.iteritems()])) # generate a dataframe from the dict. have to use this weird formulation to get around an error, not sure how it works            test['cage'] = currCage                     # force every row to have a cage number            test['day'] = str(currDay)                  # force every row to have a daystamp            df = df.append(test, ignore_index=True)     # append to final dataframe        # create real-time tracking log to demonstrate progress through cages and days            if n == 1:                print ("Processing run/rest data for session number 1 of " + str(total_n))            mod = n % 50            if mod == 0:                print ("Processing run/rest data for session number " +str(n) + " of " + str(total_n))            n += 1    # reorganize dataframe, generate velocity column, produce pivot file and save csv    df = df[['day', 'cage', 'run_start', 'run_end', 'run_intervals', 'run_dist(m)', 'rest_start', 'rest_end', 'rest_intervals']]    df.fillna(value=np.nan, inplace=True)    df['velocity m/min'] = df['run_dist(m)']/df['run_intervals']    df['velocity m/min'] = df['velocity m/min'].apply(lambda x: x/5)    df.to_csv(resultsdir+"\\run_rest_sessions_data.csv", index=None)    dfpivot = pd.pivot_table(df, index=['cage'], columns=['day'])    dfpivot.to_csv(resultsdir+"\\run_rest_sessions_pivot.csv")    print ("")######################################################################################################################################################################################################################################################################################################def hourly_binned_RW_data():    ####################################################################    #    # generate hourly binned data from the trimmed output files    #    ####################################################################    # create an appendable array for hourly bins    binned_output = pd.DataFrame()    # initialize a variable to store the path of *output.csv files to be analyzed    listoftrimmedfiles = []    for file in glob.glob(resultsdir+'\\*output.csv'):        listoftrimmedfiles.append(file)    # set RWfile to the path of the first *output.csv file    RWfile = listoftrimmedfiles[0]    # set 'for loop' to iterate across all csv files in the working director    # then open csv file using csvreader using 'with' so files are automatically closed    for RWfile in listoftrimmedfiles:            with open(RWfile) as csvfile:                reader = csv.reader(csvfile,delimiter=",",quotechar="\"")     # find the header row                for num, row in enumerate(reader):                    if 'Cage' in row:                        datarow = num     # print initial dataframe, starting at header row. Only store cage, interval # and distance                RWdf = pd.read_csv(RWfile, skiprows=datarow, usecols=[0,1,2,5])     # rename RWdf columns                RWdf.columns = ['Cage','Date','Interval','Distance']     # establish date for output data table and insert into dataframe. Capturing the date from     # the filename is an expedient way of establishing one day per night     #           expt_date= str(RWfile[-22:-14])     #           RWdf['Date'] = expt_date    # sort dataframe for iterative analysis                RWdf = RWdf.sort_values(['Cage','Interval'])    # the number of binned intervals will be the total # of 5 min data points (or rows)    # divided by 12 (since each bin is one hour, containing 12 intervals)                bin_int = (len(RWdf.index))//12     # determine the # of binned intervals as integer, not float. other option is to use tot_rows//12#                numberofcages=1 + RWdf['Cage'].max() # determine # of cages in study    # for modular math below                a = 0    # build dataframe with 11 columns across and rows = the # cages                binned = pd.DataFrame()    # set up 'for loop' to iterate through dataframe. Note that row:column starts at 0:0, not 1:1                for i in range(0,int(bin_int)):                    bin_sum = RWdf.iloc[(12*i):(12*i+12),3].sum()                    cage_num = RWdf.iloc[(12*i),0]                    cage_date = RWdf.iloc[(12*i),1]                    r,c = divmod(a,9)                    binned.set_value(r, 0, cage_num)                    binned.set_value(r, 1, cage_date)                    binned.set_value(r, c+2, bin_sum)                    a = a + 1     # append output data to binned_output file, print filename                binned_output = binned_output.append(binned)    # exit for loop, label columns, sort the output dataframe    binned_output.columns = ['Cage', 'Date','1h', '2h', '3h', '4h', '5h', '6h', '7h', '8h', '9h']    binned_output = binned_output.sort_values(['Cage','Date'])    # create and format the output file using "a" option so that data can be appended to it    writer = (resultsdir+"\\concat_hourly_binned_data.csv") # export to excel file    binned_output.to_csv(path_or_buf=writer, mode='w', index=False)    print (str(len(listoftrimmedfiles)), "RW datafiles were analyzed, binned, trimmed, reshaped and summary data exported\n")########################################################################################################################################################################################################################################################################################################################################################################### Main executable##################################################################### determine current time using datetimenow = datetime.datetime.today()runtime = datetime.datetime.strftime(now, "%m/%d/%y %H:%M:%S")basedir = os.getcwd()# communicate with userprint ("\n\n******************")print ("System date is",runtime)print ("\nSelect the directory containing the RW data files to be analyzed. No other .csv files should be present in that directory")# establish directory containing RW files to be analyzed and create directory to hold resultsrundir = eg.diropenbox(msg="select directory", title="select the directory that contains the RW files to be analyzed", default=basedir)print("\nOpening",rundir,"for analysis")resultsdir = rundir+'\\RW_results'# check to see if results directory already exists, and if not then create itif os.path.isdir(resultsdir):    print("")else:    os.mkdir(resultsdir)# update user on initial conditionsprint ("\n\n******************")print ("System date is",runtime)print ("\nThe working directory is",rundir)print ("\nAll *.csv files in the working directory should be raw RW datafiles\n")print ("Analyzed files will be placed in",resultsdir)# Run functions. The first function trims input and conducts basic statistical analysesanalyze_raw_data()# the next function generates concatenated files that only includes data from analyzes time periods.concatenate_trimmed_data()# the next function generates a list of run and rest sessionscalcSessions()# the last function bins data in 1 h intervals.hourly_binned_RW_data()print ("Note: distance data are converted to meters")print ("\nAnalysis complete\n******************")
